@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
    CTLuse_article_number = "yes",
    CTLuse_paper = "yes",
    CTLuse_forced_etal = "no",
    CTLmax_names_forced_etal = "50",
    CTLnames_show_etal = "50",
    CTLuse_alt_spacing = "yes",
    CTLalt_stretch_factor = "4",
    CTLdash_repeated_names = "yes",
    CTLname_format_string = "{f.~}{vv~}{ll}{, jj}",
    CTLname_latex_cmd = "",
    CTLname_url_prefix = "[Online]. Available:"
}

@misc{wang_esrgan_2018,
	title = {{ESRGAN}: {Enhanced} {Super}-{Resolution} {Generative} {Adversarial} {Networks}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{ESRGAN}},
	url = {https://arxiv.org/abs/1809.00219},
	doi = {10.48550/ARXIV.1809.00219},
	abstract = {The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN - network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge. The code is available at https://github.com/xinntao/ESRGAN .},
	urldate = {2025-12-08},
	publisher = {arXiv},
	author = {Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Loy, Chen Change and Qiao, Yu and Tang, Xiaoou},
	year = {2018},
	note = {Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	annote = {Other
To appear in ECCV 2018 workshop. Won Region 3 in the PIRM2018-SR Challenge. Code and models are at https://github.com/xinntao/ESRGAN},
}

@misc{liang_swinir_2021,
	title = {{SwinIR}: {Image} {Restoration} {Using} {Swin} {Transformer}},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	shorttitle = {{SwinIR}},
	url = {https://arxiv.org/abs/2108.10257},
	doi = {10.48550/ARXIV.2108.10257},
	abstract = {Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by \${\textbackslash}textbf\{up to 0.14\${\textbackslash}sim\$0.45dB\}\$, while the total number of parameters can be reduced by \${\textbackslash}textbf\{up to 67\%\}\$.},
	urldate = {2025-12-08},
	publisher = {arXiv},
	author = {Liang, Jingyun and Cao, Jiezhang and Sun, Guolei and Zhang, Kai and Van Gool, Luc and Timofte, Radu},
	year = {2021},
	note = {Version Number: 1},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Image and Video Processing (eess.IV)},
	annote = {Other
Sota results on classical/lightweight/real-world image SR, image denoising and JPEG compression artifact reduction. Code: https://github.com/JingyunLiang/SwinIR},
}

@misc{ledig_photo-realistic_2016,
	title = {Photo-{Realistic} {Single} {Image} {Super}-{Resolution} {Using} a {Generative} {Adversarial} {Network}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1609.04802},
	doi = {10.48550/ARXIV.1609.04802},
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	urldate = {2025-12-08},
	publisher = {arXiv},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	year = {2016},
	note = {Version Number: 5},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (stat.ML)},
	annote = {Other
19 pages, 15 figures, 2 tables, accepted for oral presentation at CVPR, main paper + some supplementary material},
}

@incollection{fleet_learning_2014,
	address = {Cham},
	title = {Learning a {Deep} {Convolutional} {Network} for {Image} {Super}-{Resolution}},
	volume = {8692},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-319-10592-5 978-3-319-10593-2},
	url = {http://link.springer.com/10.1007/978-3-319-10593-2_13},
	language = {en},
	urldate = {2025-12-08},
	booktitle = {Computer {Vision} â€“ {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	doi = {10.1007/978-3-319-10593-2_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {184--199},
}
